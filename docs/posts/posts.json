[
  {
    "path": "posts/2021-09-03-whats-the-difference/",
    "title": "What's the difference?",
    "description": "In the social sciences we talk a lot about differences across space and time. But how do we know which differences are meaningful?",
    "author": [
      {
        "name": "Michael Flynn",
        "url": "http://m-flynn.com"
      }
    ],
    "date": "2021-09-03",
    "categories": [],
    "contents": "\r\nIn the social sciences we talk a lot about groups being different from one another. We might also talk about things like spending on certain programs going up or down over time. While this can seem direct, comparing quantities of interest can often be made complicated by a variety of factors. Below I’ll provide an overview of some of the issues I want you to be aware of as we begin to read journal articles and other readings that will invariably discuss inter-group and inter-temporal differences.\r\nComparing two groups\r\nComparing group behaviors can be complicated because in addition to differences between the groups, there is also variation in how the individual members of those groups tend to behave. Let’s imagine that we want to compare bipartisanship in US foreign policy. There are different metrics we could use, but we’ll settle for Congressional voting patterns. We might want to know if Democrats are more or less bipartisan than Republicans. We could imagine that we’re going to sample votes taken by members of Congress where each member of Congress votes on a series of votes during each year—let’s say 100 votes—and we could code those votes as bipartisan (1) or not (0). Rather than collecting data on every vote we decide to just sample from one year. At the end we might end up with a distribution of bipartisan votes that looks like this:\r\n\r\n\r\n\r\nThe figure shows the distribution of the percentage of the 100 votes each legislator makes that are bipartisan—A value of 60 means that 60 of the 100 votes, or 60% of the 100 votes, a given legislator makes are bipartisan and 40% are not. We’ll focus on the raw count of votes rather than percent for now, but the 100 vote total simplifies this to make these voting totals more intuitive for present purposes. Dealing with proportional measures can get more complicated, but we’ll set some of that aside for now.\r\nStatistical methods for determining differences between groups\r\nSo are Republicans and Democrats different? Are they meaningfully different? How can we tell? The idea here is that when we’re dealing with data of the kind shown in the above example, we want to understand if the differences between two groups are 1) systematic or are they the result of chance, and 2) how big or substantively meaningful are the differences we observe? Making these sorts of judgements by looking at this figure alone is difficult.\r\nFor example, a simple comparison of the mean values of the two parties suggests that Democrats have a higher rate of bipartisanship than do Republicans. But the data shown above are also very noisy, they represent just a sample from a broader set of votes, and there’s quite a bit of overlap between members of each group, so we can’t be sure from casual observation that the differences we think we observe are accurate representations of broader differences between the two groups. This is where we turn to some basic statistical methods that can help us to estimate if and how two or more groups might be different from one another with respect to some outcome of interest.\r\nDifference of Means Tests\r\nThe sorts of statistical methods used by researchers vary quite a bit depending on the question at hand, but for this basic example we can start with something really simple called a difference of means test, or a t test. This test allows us to look at the values of two continuous variables and compare their mean values. Another way to think of this is to compare the values of a given variable across two groups. In our example we’re interested in comparing the mean number of bipartisan votes for Republican and Democratic legislators. So political party represents our grouping variable and the outcome of interest is the count of the number of bipartisan votes.\r\n\r\n\r\n(#tab:t test example)Difference of means test of bipartisan voting\r\n\r\n\r\nParameter1\r\n\r\n\r\nParameter2\r\n\r\n\r\nMean_1\r\n\r\n\r\nMean_2\r\n\r\n\r\nDiff\r\n\r\n\r\nCI\r\n\r\n\r\nCI_low\r\n\r\n\r\nCI_high\r\n\r\n\r\nt\r\n\r\n\r\ndf_error\r\n\r\n\r\np\r\n\r\n\r\nMethod\r\n\r\n\r\nAlternative\r\n\r\n\r\nRepublican\r\n\r\n\r\nDemocrat\r\n\r\n\r\n44.88\r\n\r\n\r\n50.51\r\n\r\n\r\n-5.63\r\n\r\n\r\n0.95\r\n\r\n\r\n-6.6\r\n\r\n\r\n-4.67\r\n\r\n\r\n-11.47\r\n\r\n\r\n406.57\r\n\r\n\r\n0\r\n\r\n\r\nWelch Two Sample t-test\r\n\r\n\r\ntwo.sided\r\n\r\n\r\nThe output in ?? resembles the output for a regression model that we’ll discuss below, but is much simpler. It tells us the names of the two groups, the mean score for each of the two groups, the difference between those mean scores, and the confidence interval, confidence interval limit values, and the test statistic or t value. There is also more information contained in the table about the type of t test and the direction of the test. Sometimes you might want to look at whether or not a group has a score that is higher or lower than another group, meaning that you have some specific direction in mind. In our case we’re content to just look at a non-directional test to see if the means of the two groups are different without hypothesizing the direction of that difference in advance. For now our basic hypothesis is that the groups are different, and our null hypothesis is that there is no difference between the two groups.\r\nSo is there a difference in the bipartisan voting behavior of the two parties? The t test shows that the Republican score is indeed lower than the Democratic score, and that this difference is statistically significant at the 0.95 level. The Democratic score is 5.63 higher than the Republican score as can be seen in the “Diff” column, and the test yields a test statistic of -11.17. This translates to a very low p value, which indicates the probability of seeing a test statistic greater than or equal to the observed test statistic of the null hypothesis were true. This is a lot, I know, but the important point is that researchers generally want low p values.\r\nSo this result is statistically significant, but is it substantively significant? That’s up to the researcher and requires some knowledge of the specific topic and/or broader domain expertise. In this example we see that, on average, Republican legislators case a bipartisan vote about 45% of the time and Democratic legislators case a bipartisan vote about 50% of the time. This this case we’re looking at a difference of approximately 5 percentage points. Again, whether or not this is substantively meaningful requires a lot of additional context, and my purpose here is less to tell you the answer to that specific question for this hypothetical and more to help you to learn to be thoughtful and cautious when consuming statistical analyses.\r\nRegression Models\r\nThe t test represents a very simple approach to analyzing the differences between two groups. While this can be useful, sometimes our data and the relationships between variables of interest are too complicated for a simple t test. In some cases we might want to know how groups compare when taking into account a number of other factors. Regression models are a broad class of statistical tools that allow us to model more complex relationships among various predictor and outcome variables. A fuller treatment of regression models is more than I want to get into for the purposes of this class, but there are a couple of points I want to highlight.\r\n\r\n\r\nTable 1: Linear Regression Predicting Bipartisan Votes\r\n\r\n\r\n\r\n\r\nModel 1\r\n\r\n\r\nRepublicans\r\n\r\n\r\n-5.634***\r\n\r\n\r\n\r\n\r\n(0.487)\r\n\r\n\r\nIntercept\r\n\r\n\r\n50.509***\r\n\r\n\r\n\r\n\r\n(0.331)\r\n\r\n\r\nNum.Obs.\r\n\r\n\r\n434\r\n\r\n\r\nR2\r\n\r\n\r\n0.236\r\n\r\n\r\nR2 Adj.\r\n\r\n\r\n0.234\r\n\r\n\r\nAIC\r\n\r\n\r\n2643.2\r\n\r\n\r\nBIC\r\n\r\n\r\n2655.4\r\n\r\n\r\nLog.Lik.\r\n\r\n\r\n-1318.600\r\n\r\n\r\nF\r\n\r\n\r\n133.598\r\n\r\n\r\n + p < 0.1, * p < 0.05, ** p < 0.01, *** p < 0.001\r\n\r\n\r\nFirst, Table 1 shows the results of a regression model where we model bipartisan votes as a function of party identification. There’s a lot here but we’re only going to focus on a couple of points.\r\nFirst, the higlighted row with the red text is really what we’re interested in when comparing the two parties as we discuss above. The left column shows us the name of the variable—in this case it’s the group we’re interested in, which is the political party of the observed legislator. The right column shows us the correlation coefficient. Basically this tells us how big the difference is between the Democratic legislators (the reference group here) and the Republican legislators. Note that the number listed here, -5.634, is the same as the number listed in the t test above!\r\nThe asterisks next to the coefficient represent the level of statistical significance associated with that coefficient. This is often used as a marker of importance, but for reasons we’ve discussed briefly above, this isn’t always a great basis on which to judge the substantive significance of a result.\r\nWhere do these stars come from? Well, the number in the parentheses below the coefficient is the standard error of the coefficient. Basically, you divide the coefficient value by the standard error (i.e. \\(-5.64/0.48\\)) and (according to some arbitrarily defined conventions) a coefficient earns another asterisk every time that resulting ratio crosses a particular threshold. Don’t worry about that too much right now.\r\nThe Intercept represents the expected value of the outcome when the other predictor variables are set to 0. In this case the intercept represented the mean value of the bipartisan voting indicator (our outcome) when the Republican variable is set to 0. In other words, this is the mean score for the Democrats! See above in the t test table to check it out (there’s some minor rounding)!\r\nThere are other details here that can be useful, like the “N” or the number of observations. This tells you how many times we’re observing the phenomenon of interest. We often want more information, but a larger N doesn’t solve all our problems if there are other serious flaws with our measurement strategy or research design.\r\nAt the bottom of the table you can often find a listing of the significance thresholds. Each symbol corresponds to a particular p value. Remember that we derive these p values from the coefficient and the standard error. You can have very very small p values for coefficients with very very small substantive effects, so this is not equivalent to the importance of a coefficient. I can’t say that enough!\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-09-03-whats-the-difference/whats-the-difference_files/figure-html5/vote-example-1-1.png",
    "last_modified": "2021-10-01T16:43:38-05:00",
    "input_file": {},
    "preview_width": 5200,
    "preview_height": 3200
  },
  {
    "path": "posts/2021-08-24-a-note-on-terminology/",
    "title": "A note on terminology",
    "description": "There can be a lot of overlap in the language used in popular discourse and by political scientists. It can be helpful to think about distinguishing between the casual and technical uses of these terms.",
    "author": [
      {
        "name": "Michael Flynn",
        "url": {}
      }
    ],
    "date": "2021-08-24",
    "categories": [],
    "contents": "\r\nIn class the other day I made note of the fact that we sometimes use terms in political science that might have a different meaning as compared to how those same terms are used when we talk about politics in a less technical setting.\r\nBias\r\nA good example of this is the concept of bias. In more casual settings we typically hear the term bias used as a way of discrediting a source of information. Usually this is because that source is viewed as selectively presenting information, or framing it in such a way, so as to advance a particular political agenda.\r\nIn political science when we use the term bias we are often referring to characteristics of a sample or measurement. More specifically, bias is usually used to refer to the gap between the information we have and the “true” underlying value that we’re interested in.\r\nFor example, if we’re interested in estimating the average height of a population we might begin by taking a sample of people from that population. Let’s say that we have a population of 1,000 people—then we might sample 100 people from that population and take their average height. We could then use this to estimate the average height of the entire population. Let’s take a look:\r\n\r\n\r\n\r\nFigure 1: Simulated height values in inches.\r\n\r\n\r\n\r\nFigure 1 shows 1000 simulated height values (in inches). We have a mean of 69 inches and a standard deviation of 3 inches. We’ll set aside whether this is accurate or not for the time being, but it’s close enough for present purposes. The purple values show 20 sample values taken from the population. We’ll also assume for present purposes that our selection process only targets a particularly young portion of the population who are between 12 and 25 years of age, and so many of the people we’ve selected are not fully grown yet. Finally, the dashed line shows the mean height of the sample group and the solid line is the mean value of the population.\r\nA couple of details emerge. First, the size of the sample is necessarily smaller than the population. This is obvious as we only chose 20 out of 1000 people. That’s why we call it a sample! Second, and most importantly for present purposes, you can see that there is a gap between the two mean values—this is the bias in our estimate!\r\nBias like this can result from a variety of sources. Using height as an example, we might find bias in our sample if the people we sample are all professional basketball players (more likely to be taller), or are all under the age of 25 (younger people are more likely to be shorter). That’s what we did in this example. We could easily do the same thing with another measure like age. If we’re interested in estimating the average age of people in a county or state, and if we only sampled individuals by drawing on people within primary and secondary schools, then our estimate of age would be biased. In this case a biased sample would produce a biased estimate.\r\nNo matter what measure you choose to focus on, you have to be cognizant of the potential sources of bias—the things that can lead you to estimate a quantity of interest that is different from the actual underlying value in which you are interested. To be clear, there will almost always be a difference—the question is how close are we to the actual value we’re interested in. Often times these problems can be lessened or even remedied by ensuring that our sample matches the characteristics of the population and/or by collecting more data (i.e. by collecting a larger sample).\r\nIn this first example you can see that most of our sampled observations are close to the mean, but this is because this is still where most of the probability mass of the distribution is located. Next, let’s see what happens if we broaden our sampling procedure to ensure we’re sampling from the entire population:\r\n\r\n\r\n\r\nFigure 2: Simulated height values in inches.\r\n\r\n\r\n\r\nThere’s still some bias, but the line moves closer to the population mean because we’ve altered our sampling procedure to ensure we’re not just targeting younger people in the population. Now we’re sampling from the entire population. But now our sample mean is above the population mean instead of below. This is a result of the small sample we’ve chosen. Smaller samples are more likely to yield noise. Now let’s see what happens when we keep everything the same but simply increase the size of the sample 10 times:\r\n\r\n\r\n\r\nFigure 3: Simulated height values and a sample of 200 individuals\r\n\r\n\r\n\r\nThe lines moved closer together, just by increasing the size of the sample! Accordingly, we have reduced the bias of our estimate. Importantly, though, there is still going to be some amount of bias. It is often not something we can ever completely eliminate, but we can try to reduce it.\r\nAccuracy and Validity\r\nTwo other terms that we’ll use that have more technical meaning are accuracy and validity. These are maybe less at odds with popular useage than bias, but it’s good to review them just to make sure we’re on the same page.\r\nWhen we’re collecting data we’re often trying to measure some phenomenon of interest. This is true whether we’re talking about qualitative data collection or quantitative data collection. Validity refers to how close a measure is to the concept that we’re interested in measuring. Reliability refers to how consistent our attempts to measure that concept or phenomenon are.\r\nFigure 3 captures this concept. If we imagine a dart board or a target, validity refers to how close to the bulls eye we get when we try to measure the thing of interest. Similarly, reliability refers to our target grouping. If we’re hitting the bulls eye, or close to it, every time we throw a dart then we’d say that our measurement has a high degree of validity and reliability. If our darts tend to center around the bulls eye but are evenly scattered around the board then then we’d say our measurement instrument is valid, but not very reliable. If our attempts to measure the thing of interest aren’t near the bulls eye and are scattered around, then we’d say our measurement instrument are neither valid nor reliable. ultimately, validity and reliability can both be high, low, or some combination of the two.\r\n\r\n\r\n\r\nFigure 4: Accuracy and validity\r\n\r\n\r\n\r\nThese concepts matter in real life in a wide range of situations. If we want to know how the public feels about a particular policy then the way we phrase a question on a survey instrument can have a huge impact on the quality of the responses with respect to the views we’re trying to tap into. If we have a medical test designed to test for the presence of a particular disease then we want to make sure that the test going to detect that disease with a high rate of success, while also minimizing the chances that it detects other diseases that we’re not interested in.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-08-24-a-note-on-terminology/a-note-on-terminology_files/figure-html5/height-sim1-1.png",
    "last_modified": "2021-08-25T10:08:48-05:00",
    "input_file": {},
    "preview_width": 1152,
    "preview_height": 1152
  },
  {
    "path": "posts/2021-08-18-test-post/",
    "title": "Pro forma introductory post",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Michael Flynn",
        "url": {}
      }
    ],
    "date": "2021-08-18",
    "categories": [],
    "contents": "\r\nThis is a test. I’ll use this blog to post announcements and supplemental material that expands on readings, lectures, discussions, etc.\r\nHere’s some R code:\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-08-18-test-post/test-post_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2021-08-18T17:12:22-05:00",
    "input_file": {},
    "preview_width": 5200,
    "preview_height": 3200
  }
]
